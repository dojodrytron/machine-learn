                                                {"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\nfrom __future__ import print_function\n#%matplotlib inline\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n# 设置随机种子\nmanualSeed = 999\n#manualSeed = random.randint(1, 10000) # 想获取新结果时使用\nprint(\"Random Seed: \", manualSeed)\nrandom.seed(manualSeed)\ntorch.manual_seed(manualSeed)\n\ndataroot = \"../input/celeba-dataset/img_align_celeba/\" #数据集的根目录\nworkers = 2 #载数据的线程数量\nbatch_size = 128 #训练过程batch大小\nimage_size = 64 #训练图片大小，所有图片均需要缩放到这个尺寸\nnc = 3 #通道数量，通常彩色图就是rgb三个值\nnz = 100 #产生网络输入向量的大小\nngf = 64 #产生网络特征层的大小\nndf = 64 #判别网络的特征层的大小\nnum_epochs = 5 #训练数据集迭代次数\nlr = 0.0002 #学习率\nbeta1 = 0.5 #Adam最优化方法中的超参 beta1\nngpu = 1 #可用的gpu数量（0为cpu模式）\n\n\n# 创建数据集（包含各种初始化）\ndataset = dset.ImageFolder(root=dataroot,\n                           transform=transforms.Compose([\n                               transforms.Resize(image_size),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n# 创建数据载入器 DataLoader\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                         shuffle=True, num_workers=workers)\n\n# 设置训练需要的处理器\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:10:37.108485Z","iopub.execute_input":"2022-06-02T02:10:37.10973Z","iopub.status.idle":"2022-06-02T02:11:16.223099Z","shell.execute_reply.started":"2022-06-02T02:10:37.109679Z","shell.execute_reply":"2022-06-02T02:11:16.222301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_batch = next(iter(dataloader))\nplt.figure(figsize=(8, 8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[: 64], padding = 2, normalize = True).cpu(), (1, 2, 0)))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:11:16.224824Z","iopub.execute_input":"2022-06-02T02:11:16.225173Z","iopub.status.idle":"2022-06-02T02:11:17.399775Z","shell.execute_reply.started":"2022-06-02T02:11:16.225137Z","shell.execute_reply":"2022-06-02T02:11:17.398255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weights_init(m): \n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02) \n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:11:17.401585Z","iopub.execute_input":"2022-06-02T02:11:17.402135Z","iopub.status.idle":"2022-06-02T02:11:17.409139Z","shell.execute_reply.started":"2022-06-02T02:11:17.402095Z","shell.execute_reply":"2022-06-02T02:11:17.408339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # 输入向量z，通过第一个反卷积\n            # 将100的向量z输入，输出channel设置为(ngf*8)，经过如下操作 \n            # class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n            # 后得到(ngf*8) x 4 x 4，即长宽为4，channel为ngf*8的特征层\n            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False), #这里的ConvTranspose2d类似于deconv，前面第 章介绍过原理\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            \n            # 继续对特征层进行反卷积，得到长宽为8，channel为ngf*4的特征层  (ngf*4) x 8 x 8\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            \n            # 继续对特征层进行反卷积，得到长宽为16，channel为ngf*2的特征层  (ngf*2) x 16 x 16\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            \n            # 继续对特征层进行反卷积，得到长宽为32，channel为ngf的特征层  (ngf) x 32 x 32\n            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            \n            # 继续对特征层进行反卷积，得到长宽为64，channel为nc的特征层  (nc) x 64 x 64\n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n################################################################\n#### 将产生网络实例化 ####\n# 创建生成器\nnetG = Generator(ngpu).to(device)\n\n# 处理多gpu情况\nif (device.type == 'cuda') and (ngpu > 1):\n    netG = nn.DataParallel(netG, list(range(ngpu)))\n\n# 应用weights_init函数对随机初始化进行重置，改为服从mean=0, stdev=0.2的正态分布的初始化\nnetG.apply(weights_init)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:11:17.411586Z","iopub.execute_input":"2022-06-02T02:11:17.412074Z","iopub.status.idle":"2022-06-02T02:11:17.461092Z","shell.execute_reply.started":"2022-06-02T02:11:17.41204Z","shell.execute_reply":"2022-06-02T02:11:17.460294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 判别网络代码\nclass Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # 输入为一张宽高为64，channel为nc的一张图片，得到宽高为32，channel为ndf的一张图片  (ndf) x 32 x 32\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 经过第2次卷积 得到宽高为16，channel为ndf*2的一张图片 (ndf*2) x 16 x 16\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2), #使用大尺度的步长来代替下采样（pooling），这样可以更好地学习降采样的方法\n            nn.LeakyReLU(0.2, inplace=True),\n            # 经过第3次卷积 得到宽高为8，channel为ndf*4的一张图片  (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # 经过第4次卷积 得到宽高为4，channel为ndf*8的一张图片  (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # 经过第5次卷积并过sigmoid层，得最终一个概率输出值\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid() #最终通过Sigmoid激活函数输出该张图片是真实图片的概率\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n#### 将判别网络实例化 ####\n# 创建判别器\nnetD = Discriminator(ngpu).to(device)\n\n# 处理多gpu情况\nif (device.type == 'cuda') and (ngpu > 1):\n    netD = nn.DataParallel(netD, list(range(ngpu)))\n    \n# 应用weights_init函数对随机初始化进行重置，改为服从mean=0, stdev=0.2的正态分布的初始化\nnetD.apply(weights_init)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:11:17.462176Z","iopub.execute_input":"2022-06-02T02:11:17.462524Z","iopub.status.idle":"2022-06-02T02:11:17.50156Z","shell.execute_reply.started":"2022-06-02T02:11:17.46249Z","shell.execute_reply":"2022-06-02T02:11:17.500783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#初始化二元交叉熵损失函数\ncriterion = nn.BCELoss()\n#创建一个batch大小的向量z，即产生网络的输入数据\nfixed_noise = torch.randn(64, nz, 1, 1, device=device)\n#定义训练过程的真图片/假图片的标签\nreal_label = 1\nfake_label = 0\n#为产生网络和判别网络设置Adam优化器\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999)) \noptimizerG = optim.Adam(netG.parameters(),lr=lr, betas=(beta1, 0.999))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:11:17.503267Z","iopub.execute_input":"2022-06-02T02:11:17.503814Z","iopub.status.idle":"2022-06-02T02:11:17.510267Z","shell.execute_reply.started":"2022-06-02T02:11:17.503779Z","shell.execute_reply":"2022-06-02T02:11:17.509585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_list = []\nG_losses = []\nD_losses = []\niters = 0\nprint(\"Starting Training Loop...\")\nfor epoch in range(num_epochs): # 训练集迭代的次数\n    for i, data in enumerate(dataloader, 0): #循环每个dataloader中的batch\n        \n        ############################\n        # (1) 更新判别网络：最大化 log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        ## 用全部都是真图片的batch训练\n        netD.zero_grad()\n        # 格式化batch\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, device=device)\n        label = label.to(torch.float32)\n        # 将带有正样本的batch，输入到判别网络 中进行前向计算，得到结果放到变量output中\n        output = netD(real_cpu).view(-1)\n        output=output.to(torch.float32)\n        # 计算loss\n        errD_real = criterion(output, label)\n\n        # 计算梯度\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        ## 用全部都是假图片的batch训练\n        # 先产生网络的输入向量\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        # 通过产生网络生成假的样本图片\n        fake = netG(noise)\n        label.fill_(fake_label)\n        # 将生成的全部假图片输入到判别网络中进行前向计算，得到结果放到变量output中\n        output = netD(fake.detach()).view(-1)\n        # 在假图片batch中计算刚刚判别网络的loss\n        errD_fake = criterion(output, label)\n        # 计算该batch的梯度\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        # 将真图片与假图片的误差加和\n        errD = errD_real + errD_fake\n        # 更新判别网络D\n        optimizerD.step()\n\n        ############################\n        # (2) 更新产生网络： 最大化 log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        label.fill_(real_label)  # 产生网络的标签是真实的图片\n        # 由于刚刚更新了判别网络，这里让假数据再过一遍判别网络，用来计算产生网络的loss并回传\n        output = netD(fake).view(-1)\n        errG = criterion(output, label)\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # 更新产生网络G\n        optimizerG.step()\n        \n        # 打印训练状态\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n        \n        # 保存loss，用于后续画图\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n        \n        # 保留产生网络生成的图片，后续用来看生成的图片效果 \n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n            \n        iters += 1\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:11:17.513069Z","iopub.execute_input":"2022-06-02T02:11:17.513479Z","iopub.status.idle":"2022-06-02T02:45:09.95168Z","shell.execute_reply.started":"2022-06-02T02:11:17.513448Z","shell.execute_reply":"2022-06-02T02:45:09.950674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####对比真实图片和产生的假图片####\nreal_batch = next(iter(dataloader)) #从dataloader中取一个batch（64个）的图片\n#画真实的图片\nplt.figure(figsize=(15,15))\nplt.subplot(1,2,1)\nplt.axis(\"off\")\nplt.title(\"Real Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n#画出产生网络最后一个迭代产生的图片\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(img_list[-1],(1,2,0)))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:59:35.997072Z","iopub.execute_input":"2022-06-02T02:59:35.997481Z","iopub.status.idle":"2022-06-02T02:59:37.650197Z","shell.execute_reply.started":"2022-06-02T02:59:35.997447Z","shell.execute_reply":"2022-06-02T02:59:37.649302Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
